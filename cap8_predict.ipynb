{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_loader import load_names_from_web, holdout_split\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfraw = load_names_from_web(category='national', hide_pre_1937=True, use_existing_files=True)\n",
    "traintestval, holdout = holdout_split(dfraw)\n",
    "trainval, test = holdout_split(traintestval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>M/F</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>2000</td>\n",
       "      <td>Madison</td>\n",
       "      <td>F</td>\n",
       "      <td>19968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>2000</td>\n",
       "      <td>Olivia</td>\n",
       "      <td>F</td>\n",
       "      <td>12854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>2000</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>F</td>\n",
       "      <td>10244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>2000</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>F</td>\n",
       "      <td>9389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>2000</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>F</td>\n",
       "      <td>8557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212633</th>\n",
       "      <td>US</td>\n",
       "      <td>2019</td>\n",
       "      <td>Zaheen</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212634</th>\n",
       "      <td>US</td>\n",
       "      <td>2019</td>\n",
       "      <td>Zahi</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212635</th>\n",
       "      <td>US</td>\n",
       "      <td>2019</td>\n",
       "      <td>Zaymar</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212636</th>\n",
       "      <td>US</td>\n",
       "      <td>2019</td>\n",
       "      <td>Zeo</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212637</th>\n",
       "      <td>US</td>\n",
       "      <td>2019</td>\n",
       "      <td>Zeshan</td>\n",
       "      <td>M</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212638 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       state  year      name M/F  count\n",
       "0         US  2000   Madison   F  19968\n",
       "1         US  2000    Olivia   F  12854\n",
       "2         US  2000    Sydney   F  10244\n",
       "3         US  2000  Jennifer   F   9389\n",
       "4         US  2000    Amanda   F   8557\n",
       "...      ...   ...       ...  ..    ...\n",
       "212633    US  2019    Zaheen   M      5\n",
       "212634    US  2019      Zahi   M      5\n",
       "212635    US  2019    Zaymar   M      5\n",
       "212636    US  2019       Zeo   M      5\n",
       "212637    US  2019    Zeshan   M      5\n",
       "\n",
       "[212638 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_known_names(data):\n",
    "\n",
    "    names = data.groupby(['state', 'name', 'M/F']).size().reset_index()\n",
    "    names = names[['state', 'name', 'M/F']]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_names(data, first_year_to_predict, cutoff=10):\n",
    "    '''\n",
    "    Select names that have had at least one year at or above the cutoff count?\n",
    "    Could also try to redo this to do the cutoff for the most recent known year's data\n",
    "    '''\n",
    "\n",
    "    data_noleak = data[data['year'] < first_year_to_predict]\n",
    "    names = data_noleak[data_noleak['count'] >= cutoff][['state', 'name', 'M/F']].drop_duplicates()\n",
    "    # display(names)\n",
    "    data = names.merge(data, how='left', on=['state', 'name', 'M/F'])\n",
    "    # display(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictor, data_held_out, first_year_to_predict, metric='msle'):\n",
    "\n",
    "    # display('data_held_out:')\n",
    "    # display(data_held_out)\n",
    "\n",
    "    most_recent_year = data_held_out['year'].max()\n",
    "    \n",
    "    years_to_predict = range(first_year_to_predict, most_recent_year+1)\n",
    "\n",
    "    # only allow the model to see data from before the year to predict\n",
    "    historical_data = data_held_out[data_held_out['year'] < first_year_to_predict]\n",
    "\n",
    "    # display('historical_data:')\n",
    "    # display(historical_data)\n",
    "\n",
    "    # get our model's predictions\n",
    "    predictions = predictor.predict(historical_data, years_to_predict)\n",
    "\n",
    "    all_known_names = get_all_known_names(historical_data)\n",
    "\n",
    "    for year_to_predict in years_to_predict:\n",
    "\n",
    "        print(f'Predictions for {year_to_predict}:')\n",
    "\n",
    "        names_to_predict = all_known_names.copy()\n",
    "        names_to_predict['year'] = year_to_predict\n",
    "        # display(names_to_predict)\n",
    "\n",
    "        observed = names_to_predict.merge(data_held_out, how='left', on=['state', 'name', 'M/F', 'year'])\n",
    "\n",
    "        # for now, fill in missing values with 2, same as FiveThirtyEight did;\n",
    "        # reasoning: missing values could be 0 to 4, so average is 2\n",
    "        observed['y'] = observed['count'].fillna(2)\n",
    "        # observed = observed.rename(columns={'count': 'count_true'})\n",
    "        # display(observed)\n",
    "\n",
    "        score_df = observed.merge(predictions, how='left', on=['state', 'name', 'M/F', 'year'], suffixes=('_true', '_pred'))\n",
    "        # display(score_df)\n",
    "\n",
    "        y_true = score_df['y_true']\n",
    "        y_pred = score_df['y_pred']\n",
    "\n",
    "        if metric == 'msle':\n",
    "            loss = mean_squared_log_error(y_true, y_pred)\n",
    "            print(f'Loss: {loss}')\n",
    "\n",
    "        if metric == 'rank':\n",
    "            y_true = y_true.rank()\n",
    "            y_pred = y_pred.rank()\n",
    "            score = np.sum(np.abs(y_true-y_pred))/(len(y_true)*(len(y_true)-1))\n",
    "            print(f'Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyPredictor():\n",
    "\n",
    "    def __init__(self, strategy='naive'):\n",
    "        self.strategy = strategy\n",
    "    \n",
    "    def predict(self, historical_data, years_to_predict):\n",
    "\n",
    "        all_known_names = get_all_known_names(historical_data)\n",
    "\n",
    "        predictions = []\n",
    "        previous_year_data = historical_data[historical_data['year'] == years_to_predict[0] - 1].drop(columns=['year'])\n",
    "\n",
    "        for year_to_predict in years_to_predict:\n",
    "\n",
    "            prediction = all_known_names.copy()\n",
    "            prediction['year'] = year_to_predict\n",
    "\n",
    "            if self.strategy == 'naive':\n",
    "                prediction = prediction.merge(previous_year_data, how='left', on=['state', 'name', 'M/F'])\n",
    "                prediction['y'] = prediction['count'].fillna(2)\n",
    "                # display(prediction)\n",
    "            elif self.strategy == 'mean':\n",
    "                prediction['y'] = previous_year_data['count'].mean()\n",
    "                # display(prediction)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        predictions = pd.concat(predictions, ignore_index=True)\n",
    "        # display(predictions)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for 2003:\n",
      "Score: 0.03204052376706701\n",
      "Predictions for 2004:\n",
      "Score: 0.040194518743844804\n",
      "Predictions for 2005:\n",
      "Score: 0.05132340343700285\n",
      "Predictions for 2006:\n",
      "Score: 0.058113785889524074\n",
      "Predictions for 2007:\n",
      "Score: 0.06422673263287938\n",
      "Predictions for 2008:\n",
      "Score: 0.07240549407644388\n",
      "Predictions for 2009:\n",
      "Score: 0.080627961049632\n",
      "Predictions for 2010:\n",
      "Score: 0.08712843015564996\n",
      "Predictions for 2011:\n",
      "Score: 0.09472299435324558\n",
      "Predictions for 2012:\n",
      "Score: 0.10091606790091082\n",
      "Predictions for 2013:\n",
      "Score: 0.10854122596924297\n",
      "Predictions for 2014:\n",
      "Score: 0.11577303427096262\n",
      "Predictions for 2015:\n",
      "Score: 0.1199877041776659\n",
      "Predictions for 2016:\n",
      "Score: 0.1251303881633771\n",
      "Predictions for 2017:\n",
      "Score: 0.13105540112935088\n",
      "Predictions for 2018:\n",
      "Score: 0.13403029084573115\n",
      "Predictions for 2019:\n",
      "Score: 0.13911033023898184\n",
      "Predictions for 2020:\n",
      "Score: 0.1412052819589401\n",
      "Predictions for 2021:\n",
      "Score: 0.1478150148890171\n",
      "Predictions for 2022:\n",
      "Score: 0.15419310851208895\n"
     ]
    }
   ],
   "source": [
    "first_year_to_predict = 2003\n",
    "cutoff = 100\n",
    "# data_to_fit = select_top_names(trainval, first_year_to_predict=first_year_to_predict, cutoff=0)\n",
    "data_to_eval = select_top_names(test, first_year_to_predict=first_year_to_predict, cutoff=cutoff)\n",
    "evaluate(predictor=DummyPredictor(strategy='naive'), data_held_out=data_to_eval, first_year_to_predict=first_year_to_predict, metric='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "class MyPredictor():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # params\n",
    "        cols_to_keep = ['M/F', 'sum', 'median_age', 'thisyear_count']\n",
    "        categorical_features = ['M/F']\n",
    "        max_leaf_nodes = 16 # 16\n",
    "        max_iter = 100 # 100\n",
    "        loss = 'absolute_error' # abs better than default\n",
    "\n",
    "        categorical_features = [True if f in categorical_features else False for f in cols_to_keep]\n",
    "        # print(categorical_features)\n",
    "        \n",
    "        self.pipe = make_pipeline(\n",
    "            ColumnTransformer(\n",
    "                transformers=[\n",
    "                    # ('category_encoder', LabelEncoder(), categorical_features),\n",
    "                    ('cols_to_keep', 'passthrough', cols_to_keep),\n",
    "                ], remainder='drop'),\n",
    "            HistGradientBoostingRegressor(\n",
    "                random_state=0,\n",
    "                categorical_features=categorical_features,\n",
    "                max_leaf_nodes=max_leaf_nodes,\n",
    "                max_iter=max_iter,\n",
    "                loss=loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.gender_encoding = {'M': 0, 'F': 1}\n",
    "\n",
    "    def preprocess(self, df, latest_known_year):\n",
    "\n",
    "        # find median age of people with name, \n",
    "        # total born with that name,\n",
    "        # and latest year's count\n",
    "\n",
    "        df = df.copy()\n",
    "        df = df.sort_values(by='year')\n",
    "        df['cumsum'] = df.groupby(['state', 'name', 'M/F'])['count'].cumsum()\n",
    "        df['sum'] = df.groupby(['state', 'name', 'M/F'])['count'].transform('sum')\n",
    "        # display(df[(df['name'] == 'Millie') & (df['M/F'] == 'M')])\n",
    "\n",
    "        medians = df[df['cumsum'] >= df['sum']/2]\n",
    "        medians = medians.drop_duplicates(subset=['state', 'name', 'M/F'], keep='first')\n",
    "        medians['median_age'] = latest_known_year - medians['year']\n",
    "        # display(medians[medians['name'] == 'Madison'])\n",
    "\n",
    "        thisyear = df[df['year'] == latest_known_year][['state', 'name', 'M/F', 'count']].rename(columns={'count': 'thisyear_count'})\n",
    "        df2 = medians.merge(thisyear, how='left', on=['state', 'name', 'M/F']).rename(columns={'year': 'median_year'})\n",
    "        df2['thisyear_count'] = df2['thisyear_count'].fillna(0) # might want to shift this to 2 and fill in 2s for missing years? or maybe not\n",
    "        # display(df2)\n",
    "        # display(df2.groupby(['state','name','M/F']).ngroups)\n",
    "\n",
    "        # change M/F to 0/1 so it works with various models\n",
    "        # (even HistGradientBoostingRegressor, which accepts categorical values,\n",
    "        # still needs those values to be numbers not strings)\n",
    "        df2['M/F'] = df2['M/F'].map(self.gender_encoding)\n",
    "\n",
    "        return df2\n",
    "    \n",
    "    def fit(self, historical_data, first_year_to_predict, years_to_fit=1, weight_decay=0.9):\n",
    "        # first things first, we don't want to know about future data\n",
    "        historical_data = historical_data[historical_data['year'] < first_year_to_predict]\n",
    "        # at this point the data we don't want to know should be inaccessible\n",
    "\n",
    "        X_all = pd.DataFrame()\n",
    "        y_all = pd.Series()\n",
    "\n",
    "        # each year_to_fit is the year that's essentially our y for that loop\n",
    "        for year_to_fit in range(first_year_to_predict - years_to_fit, first_year_to_predict):\n",
    "\n",
    "            # now we \"know\" even less for X\n",
    "            X = historical_data[historical_data['year'] < year_to_fit]\n",
    "            y = historical_data[historical_data['year'] == year_to_fit]\n",
    "\n",
    "            X = self.preprocess(X, year_to_fit - 1)\n",
    "            y = y[['state', 'name', 'M/F', 'count']].rename(columns={'count': 'y'})\n",
    "            y['M/F'] = y['M/F'].map(self.gender_encoding)\n",
    "\n",
    "            data = X.merge(y, how='left', on=['state', 'name', 'M/F'])\n",
    "            data['y'] = data['y'].fillna(0)\n",
    "            # display(data)\n",
    "\n",
    "            X = data.drop(columns=['y'])\n",
    "            y = data['y']\n",
    "            X['sample_weight'] = weight_decay ** (first_year_to_predict - year_to_fit)\n",
    "\n",
    "            X_all = pd.concat([X_all, X], ignore_index=True)\n",
    "            y_all = pd.concat([y_all, y], ignore_index=True)\n",
    "        \n",
    "        temp = X_all.copy()\n",
    "        temp['y'] = y_all\n",
    "        display(temp)\n",
    "\n",
    "        sample_weights = X_all['sample_weight']\n",
    "        X_all = X_all.drop(columns=['sample_weight'])\n",
    "\n",
    "        self.pipe.fit(X_all, y_all, **{'histgradientboostingregressor__sample_weight': sample_weights})\n",
    "        # this seems like a silly way to pass params to individual steps of the pipeline, but it's true. See: https://stackoverflow.com/questions/36205850/sklearn-pipeline-applying-sample-weights-after-applying-a-polynomial-feature-t\n",
    "\n",
    "    def predict(self, historical_data, years_to_predict):\n",
    "\n",
    "        # all_known_names = get_all_known_names(historical_data)\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        # display('historical_data in predict:')\n",
    "        # display(historical_data)\n",
    "        # display('years_to_predict:')\n",
    "        # display(years_to_predict)\n",
    "\n",
    "        for year_to_predict in years_to_predict:\n",
    "\n",
    "            # display('historical_data in predict loop:')\n",
    "            # display(historical_data)\n",
    "\n",
    "            df = self.preprocess(historical_data, year_to_predict - 1)\n",
    "            # df = self.preprocess(historical_data, years_to_predict[0] - 1)\n",
    "\n",
    "            df['y'] = self.pipe.predict(df)\n",
    "\n",
    "            df['year'] = year_to_predict\n",
    "            # display(df)\n",
    "\n",
    "            # if we want to simply, do the following; \n",
    "            # but for now, might be useful to see all data displayed.\n",
    "            # df = df[['state', 'year', 'name', 'M/F', 'y']]\n",
    "\n",
    "            predictions.append(df)\n",
    "\n",
    "            assumed_new_year_of_historical_data = df[['state', 'year', 'name', 'M/F', 'y']].rename(columns={'y': 'count'})\n",
    "            assumed_new_year_of_historical_data['M/F'] = assumed_new_year_of_historical_data['M/F'].map({v: k for k, v in self.gender_encoding.items()})\n",
    "            historical_data = pd.concat([historical_data, assumed_new_year_of_historical_data], ignore_index=True)\n",
    "\n",
    "        predictions = pd.concat(predictions, ignore_index=True)\n",
    "\n",
    "        # we have to reverse the mapping to send our predictions\n",
    "        # (at least the way we currently have it set up)\n",
    "        predictions['M/F'] = predictions['M/F'].map({v: k for k, v in self.gender_encoding.items()})\n",
    "\n",
    "        predictions.loc[predictions['y'] < 4.5, 'y'] = 2\n",
    "\n",
    "        display(predictions)\n",
    "        display(predictions[predictions['y'] < 4.5])\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>M/F</th>\n",
       "      <th>median_year</th>\n",
       "      <th>count</th>\n",
       "      <th>cumsum</th>\n",
       "      <th>sum</th>\n",
       "      <th>median_age</th>\n",
       "      <th>thisyear_count</th>\n",
       "      <th>sample_weight</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>Cloe</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>Harlie</td>\n",
       "      <td>1</td>\n",
       "      <td>1940</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Gracey</td>\n",
       "      <td>1</td>\n",
       "      <td>1941</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>Rowan</td>\n",
       "      <td>1</td>\n",
       "      <td>1942</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>Melvyn</td>\n",
       "      <td>0</td>\n",
       "      <td>1942</td>\n",
       "      <td>282</td>\n",
       "      <td>1613</td>\n",
       "      <td>2811</td>\n",
       "      <td>30</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.042391</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93592</th>\n",
       "      <td>US</td>\n",
       "      <td>Adamari</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>130</td>\n",
       "      <td>240</td>\n",
       "      <td>240</td>\n",
       "      <td>0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93593</th>\n",
       "      <td>US</td>\n",
       "      <td>Maximus</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>892</td>\n",
       "      <td>1114</td>\n",
       "      <td>1114</td>\n",
       "      <td>0</td>\n",
       "      <td>892.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>912.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93594</th>\n",
       "      <td>US</td>\n",
       "      <td>Jersey</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>61</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93595</th>\n",
       "      <td>US</td>\n",
       "      <td>Dasani</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>94</td>\n",
       "      <td>161</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93596</th>\n",
       "      <td>US</td>\n",
       "      <td>Omarion</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>418.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93597 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      state     name  M/F  median_year  count  cumsum   sum  median_age  \\\n",
       "0        US     Cloe    1         1940      8      26    42          32   \n",
       "1        US   Harlie    1         1940      5       5     5          32   \n",
       "2        US   Gracey    1         1941      7       7    12          31   \n",
       "3        US    Rowan    1         1942      7       7     7          30   \n",
       "4        US   Melvyn    0         1942    282    1613  2811          30   \n",
       "...     ...      ...  ...          ...    ...     ...   ...         ...   \n",
       "93592    US  Adamari    1         2001    130     240   240           0   \n",
       "93593    US  Maximus    0         2001    892    1114  1114           0   \n",
       "93594    US   Jersey    1         2001     61      88    88           0   \n",
       "93595    US   Dasani    1         2001     94     161   161           0   \n",
       "93596    US  Omarion    0         2001      5       5     5           0   \n",
       "\n",
       "       thisyear_count  sample_weight      y  \n",
       "0                 0.0       0.042391    0.0  \n",
       "1                 0.0       0.042391    0.0  \n",
       "2                 0.0       0.042391    0.0  \n",
       "3                 0.0       0.042391    0.0  \n",
       "4                 7.0       0.042391   14.0  \n",
       "...               ...            ...    ...  \n",
       "93592           130.0       0.900000  168.0  \n",
       "93593           892.0       0.900000  912.0  \n",
       "93594            61.0       0.900000  110.0  \n",
       "93595            94.0       0.900000  105.0  \n",
       "93596             5.0       0.900000  418.0  \n",
       "\n",
       "[93597 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>M/F</th>\n",
       "      <th>median_year</th>\n",
       "      <th>count</th>\n",
       "      <th>cumsum</th>\n",
       "      <th>sum</th>\n",
       "      <th>median_age</th>\n",
       "      <th>thisyear_count</th>\n",
       "      <th>y</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>Gertrude</td>\n",
       "      <td>F</td>\n",
       "      <td>1945</td>\n",
       "      <td>664.000000</td>\n",
       "      <td>8864.000000</td>\n",
       "      <td>17407.000000</td>\n",
       "      <td>57</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>10.309556</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>Flossie</td>\n",
       "      <td>F</td>\n",
       "      <td>1946</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>1532.000000</td>\n",
       "      <td>2988.000000</td>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Eula</td>\n",
       "      <td>F</td>\n",
       "      <td>1946</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>5131.000000</td>\n",
       "      <td>9796.000000</td>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>Olive</td>\n",
       "      <td>F</td>\n",
       "      <td>1946</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>1847.000000</td>\n",
       "      <td>3510.000000</td>\n",
       "      <td>56</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>38.912649</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>Marlys</td>\n",
       "      <td>F</td>\n",
       "      <td>1946</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>2902.000000</td>\n",
       "      <td>5565.000000</td>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16575</th>\n",
       "      <td>US</td>\n",
       "      <td>Memphis</td>\n",
       "      <td>M</td>\n",
       "      <td>2011</td>\n",
       "      <td>151.726435</td>\n",
       "      <td>1695.266727</td>\n",
       "      <td>3187.517440</td>\n",
       "      <td>10</td>\n",
       "      <td>146.769696</td>\n",
       "      <td>145.490800</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16576</th>\n",
       "      <td>US</td>\n",
       "      <td>Charlize</td>\n",
       "      <td>F</td>\n",
       "      <td>2012</td>\n",
       "      <td>326.560429</td>\n",
       "      <td>3397.273694</td>\n",
       "      <td>6287.194733</td>\n",
       "      <td>9</td>\n",
       "      <td>317.798428</td>\n",
       "      <td>312.392126</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16577</th>\n",
       "      <td>US</td>\n",
       "      <td>Kadejah</td>\n",
       "      <td>F</td>\n",
       "      <td>2013</td>\n",
       "      <td>52.571746</td>\n",
       "      <td>663.468476</td>\n",
       "      <td>1273.727742</td>\n",
       "      <td>8</td>\n",
       "      <td>92.935179</td>\n",
       "      <td>94.783283</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16578</th>\n",
       "      <td>US</td>\n",
       "      <td>Diquan</td>\n",
       "      <td>M</td>\n",
       "      <td>2013</td>\n",
       "      <td>45.249426</td>\n",
       "      <td>612.373570</td>\n",
       "      <td>1199.111111</td>\n",
       "      <td>8</td>\n",
       "      <td>93.449667</td>\n",
       "      <td>96.434158</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16579</th>\n",
       "      <td>US</td>\n",
       "      <td>Marimar</td>\n",
       "      <td>F</td>\n",
       "      <td>2014</td>\n",
       "      <td>52.494165</td>\n",
       "      <td>559.815915</td>\n",
       "      <td>1096.210119</td>\n",
       "      <td>7</td>\n",
       "      <td>93.683513</td>\n",
       "      <td>94.828440</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16580 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      state      name M/F  median_year       count       cumsum           sum  \\\n",
       "0        US  Gertrude   F         1945  664.000000  8864.000000  17407.000000   \n",
       "1        US   Flossie   F         1946  108.000000  1532.000000   2988.000000   \n",
       "2        US      Eula   F         1946  380.000000  5131.000000   9796.000000   \n",
       "3        US     Olive   F         1946  123.000000  1847.000000   3510.000000   \n",
       "4        US    Marlys   F         1946  243.000000  2902.000000   5565.000000   \n",
       "...     ...       ...  ..          ...         ...          ...           ...   \n",
       "16575    US   Memphis   M         2011  151.726435  1695.266727   3187.517440   \n",
       "16576    US  Charlize   F         2012  326.560429  3397.273694   6287.194733   \n",
       "16577    US   Kadejah   F         2013   52.571746   663.468476   1273.727742   \n",
       "16578    US    Diquan   M         2013   45.249426   612.373570   1199.111111   \n",
       "16579    US   Marimar   F         2014   52.494165   559.815915   1096.210119   \n",
       "\n",
       "       median_age  thisyear_count           y  year  \n",
       "0              57       11.000000   10.309556  2003  \n",
       "1              56        0.000000    2.000000  2003  \n",
       "2              56        0.000000    2.000000  2003  \n",
       "3              56       43.000000   38.912649  2003  \n",
       "4              56        0.000000    2.000000  2003  \n",
       "...           ...             ...         ...   ...  \n",
       "16575          10      146.769696  145.490800  2022  \n",
       "16576           9      317.798428  312.392126  2022  \n",
       "16577           8       92.935179   94.783283  2022  \n",
       "16578           8       93.449667   96.434158  2022  \n",
       "16579           7       93.683513   94.828440  2022  \n",
       "\n",
       "[16580 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>name</th>\n",
       "      <th>M/F</th>\n",
       "      <th>median_year</th>\n",
       "      <th>count</th>\n",
       "      <th>cumsum</th>\n",
       "      <th>sum</th>\n",
       "      <th>median_age</th>\n",
       "      <th>thisyear_count</th>\n",
       "      <th>y</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>Flossie</td>\n",
       "      <td>F</td>\n",
       "      <td>1946</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1532.0</td>\n",
       "      <td>2988.000000</td>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>Eula</td>\n",
       "      <td>F</td>\n",
       "      <td>1946</td>\n",
       "      <td>380.0</td>\n",
       "      <td>5131.0</td>\n",
       "      <td>9796.000000</td>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>Marlys</td>\n",
       "      <td>F</td>\n",
       "      <td>1946</td>\n",
       "      <td>243.0</td>\n",
       "      <td>2902.0</td>\n",
       "      <td>5565.000000</td>\n",
       "      <td>56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US</td>\n",
       "      <td>Virgie</td>\n",
       "      <td>F</td>\n",
       "      <td>1947</td>\n",
       "      <td>188.0</td>\n",
       "      <td>2654.0</td>\n",
       "      <td>5216.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>US</td>\n",
       "      <td>Willie</td>\n",
       "      <td>F</td>\n",
       "      <td>1947</td>\n",
       "      <td>1684.0</td>\n",
       "      <td>21064.0</td>\n",
       "      <td>39570.000000</td>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14936</th>\n",
       "      <td>US</td>\n",
       "      <td>Gene</td>\n",
       "      <td>F</td>\n",
       "      <td>1949</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>2443.006201</td>\n",
       "      <td>71</td>\n",
       "      <td>3.885710</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14979</th>\n",
       "      <td>US</td>\n",
       "      <td>Vonnie</td>\n",
       "      <td>F</td>\n",
       "      <td>1954</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>2546.938881</td>\n",
       "      <td>66</td>\n",
       "      <td>3.823917</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15753</th>\n",
       "      <td>US</td>\n",
       "      <td>Eula</td>\n",
       "      <td>F</td>\n",
       "      <td>1946</td>\n",
       "      <td>380.0</td>\n",
       "      <td>5131.0</td>\n",
       "      <td>9876.892634</td>\n",
       "      <td>75</td>\n",
       "      <td>4.438106</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15770</th>\n",
       "      <td>US</td>\n",
       "      <td>Gene</td>\n",
       "      <td>F</td>\n",
       "      <td>1949</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>2446.891910</td>\n",
       "      <td>72</td>\n",
       "      <td>3.885710</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15813</th>\n",
       "      <td>US</td>\n",
       "      <td>Vonnie</td>\n",
       "      <td>F</td>\n",
       "      <td>1954</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1288.0</td>\n",
       "      <td>2550.762798</td>\n",
       "      <td>67</td>\n",
       "      <td>3.823917</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      state     name M/F  median_year   count   cumsum           sum  \\\n",
       "1        US  Flossie   F         1946   108.0   1532.0   2988.000000   \n",
       "2        US     Eula   F         1946   380.0   5131.0   9796.000000   \n",
       "4        US   Marlys   F         1946   243.0   2902.0   5565.000000   \n",
       "8        US   Virgie   F         1947   188.0   2654.0   5216.000000   \n",
       "10       US   Willie   F         1947  1684.0  21064.0  39570.000000   \n",
       "...     ...      ...  ..          ...     ...      ...           ...   \n",
       "14936    US     Gene   F         1949    95.0   1232.0   2443.006201   \n",
       "14979    US   Vonnie   F         1954    83.0   1288.0   2546.938881   \n",
       "15753    US     Eula   F         1946   380.0   5131.0   9876.892634   \n",
       "15770    US     Gene   F         1949    95.0   1232.0   2446.891910   \n",
       "15813    US   Vonnie   F         1954    83.0   1288.0   2550.762798   \n",
       "\n",
       "       median_age  thisyear_count    y  year  \n",
       "1              56        0.000000  2.0  2003  \n",
       "2              56        0.000000  2.0  2003  \n",
       "4              56        0.000000  2.0  2003  \n",
       "8              55        0.000000  2.0  2003  \n",
       "10             55        0.000000  2.0  2003  \n",
       "...           ...             ...  ...   ...  \n",
       "14936          71        3.885710  2.0  2021  \n",
       "14979          66        3.823917  2.0  2021  \n",
       "15753          75        4.438106  2.0  2022  \n",
       "15770          72        3.885710  2.0  2022  \n",
       "15813          67        3.823917  2.0  2022  \n",
       "\n",
       "[72 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for 2003:\n",
      "Score: 0.031695250083040506\n",
      "Predictions for 2004:\n",
      "Score: 0.03898970297722068\n",
      "Predictions for 2005:\n",
      "Score: 0.04927215724666818\n",
      "Predictions for 2006:\n",
      "Score: 0.055989697149816726\n",
      "Predictions for 2007:\n",
      "Score: 0.06263585135458005\n",
      "Predictions for 2008:\n",
      "Score: 0.07140463744806326\n",
      "Predictions for 2009:\n",
      "Score: 0.07912303397959243\n",
      "Predictions for 2010:\n",
      "Score: 0.08568469082708344\n",
      "Predictions for 2011:\n",
      "Score: 0.09351817858662144\n",
      "Predictions for 2012:\n",
      "Score: 0.10028525142334341\n",
      "Predictions for 2013:\n",
      "Score: 0.10885007837858313\n",
      "Predictions for 2014:\n",
      "Score: 0.11635723151693152\n",
      "Predictions for 2015:\n",
      "Score: 0.12087346957803768\n",
      "Predictions for 2016:\n",
      "Score: 0.12723845154222246\n",
      "Predictions for 2017:\n",
      "Score: 0.13193825282774777\n",
      "Predictions for 2018:\n",
      "Score: 0.1347660005943952\n",
      "Predictions for 2019:\n",
      "Score: 0.1396202280845906\n",
      "Predictions for 2020:\n",
      "Score: 0.1422163365442329\n",
      "Predictions for 2021:\n",
      "Score: 0.146828726770511\n",
      "Predictions for 2022:\n",
      "Score: 0.15365553049771857\n"
     ]
    }
   ],
   "source": [
    "first_year_to_predict = 2003\n",
    "cutoff = 100\n",
    "my_predictor = MyPredictor()\n",
    "data_to_fit = select_top_names(trainval, first_year_to_predict=first_year_to_predict, cutoff=cutoff)\n",
    "data_to_eval = select_top_names(test, first_year_to_predict=first_year_to_predict, cutoff=cutoff)\n",
    "my_predictor.fit(historical_data=data_to_fit, first_year_to_predict=first_year_to_predict, years_to_fit=30)\n",
    "evaluate(predictor=my_predictor, data_held_out=data_to_eval, first_year_to_predict=first_year_to_predict, metric='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
